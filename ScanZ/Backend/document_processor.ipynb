{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da23aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary packages to execute functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8766fd46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import docx\n",
    "import spacy\n",
    "import nltk\n",
    "import json\n",
    "import pdfplumber\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a4cbb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vishw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26cb8708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the spacy to en_core_web_sm for english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ea53be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "584b52ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision d769bba (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\vishw\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vishw\\anaconda3\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\vishw\\anaconda3\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Load transformer-based summarization model\n",
    "\n",
    "\n",
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63c3502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels from JSON file\n",
    "file_json=\"E:/AIDocAnalyzer/legal_docs.json\"\n",
    "def load_labels_from_json(file_json):\n",
    "    with open(file_json, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "116c5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that reads a word document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8430d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word_document(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b829769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read a pdf document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2a0e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_document(file_path):\n",
    "    full_text = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            full_text.append(page.extract_text())\n",
    "    return '\\n'.join(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f30175c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarizing document using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f155cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarize the document using NLP\n",
    "def summarize_document(content, max_length=500):\n",
    "    # Limit the input size for the model to handle\n",
    "    if len(content.split()) > 512:\n",
    "        content = ' '.join(content.split()[:512])  # Trim the content if it's too long\n",
    "    # Summarize using the transformers model\n",
    "    summary = summarizer(content, max_length=max_length, min_length=30, do_sample=False)\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8e5050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to classify the type of document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "127b4f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify the document using labels from JSON\n",
    "def classify_document(content, labels):\n",
    "    # Lowercase the content for case insensitive comparison\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    for entry in labels:\n",
    "        clause_text = entry.get('clause_text', None)  # Safely get clause_text\n",
    "        if clause_text and clause_text.lower() in content_lower:  # Check if clause_text is not None\n",
    "            return entry['clause_type']\n",
    "    \n",
    "    return \"Unknown Document Type\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b910c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to return the file type\n",
    "def get_file_extension(file_path):\n",
    "    # Get the file extension and remove the leading dot\n",
    "    _, extension = os.path.splitext(file_path)\n",
    "    return extension[1:] # Slice to remove the dot\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b312492",
   "metadata": {},
   "source": [
    "# Main function to process the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28da8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_legal_document(file_path, labels_file):\n",
    "    # Load labels from JSON\n",
    "    labels = load_labels_from_json(labels_file)\n",
    "\n",
    "    if file_path.endswith('.docx'):\n",
    "        content = read_word_document(file_path)\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        content = read_pdf_document(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please provide a .docx or .pdf file.\")\n",
    "\n",
    "    # Summarize the document\n",
    "    summary = summarize_document(content)\n",
    "\n",
    "    # Classify the document using JSON labels\n",
    "    document_type = classify_document(content, labels)\n",
    "    \n",
    "    #classify the file type\n",
    "    extension = get_file_extension(file_path)\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"document_type\": document_type,\n",
    "        \"extension\":extension\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6fd94d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Summary:\n",
      "this document was created using a template from Docular (https://docular.net) you must retain the above credit . use of this document without the credit is an infringement of copyright . however, you can purchase from us an equivalent document that does not include the credit.\n",
      "\n",
      "Document Type:\n",
      "Miscellaneous\n",
      "\n",
      "File Type:\n",
      "pdf\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Replace with the path to your Word document and JSON file\n",
    "    file_path = 'E:/AIDocAnalyzer/legal_document_testpdf1.pdf'\n",
    "    labels_file = 'E:/AIDocAnalyzer/Backend/legal_docs.json'\n",
    "    \n",
    "    # Process the document\n",
    "    report = process_legal_document(file_path, labels_file)\n",
    "    \n",
    "    # Output the results\n",
    "    print(\"Document Summary:\")\n",
    "    print(report[\"summary\"])\n",
    "    print(\"\\nDocument Type:\")\n",
    "    print(report[\"document_type\"])\n",
    "    print(\"\\nFile Type:\")\n",
    "    print(report[\"extension\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
